## RL과 ML의 패러다임 비교

#### 지도학습과의 비교

- 지도학습은 학습데이터를 기반으로 학습
- 모델을 일반화 하는게 핵심
- external supervisor가 있음
  - 예를들어 강아지에게 공놀이 시키는것을 알려주는거
  - RL에서는 단지 공을 던지기만 하고 agent에게 알려주지 않음

#### 비지도 학습과의 비교

- RL은 비지도학습의 한 종류가 아님!!
- RL은 단지 reward 최대화 하는것, 반대로 비지도학습은 iddent structure를 학습
- ex) 영화 추천시 선호 영화분류는 비지도 학습, 피드백을 받고 더 알맞은 영화 추천은 RL

# RL의 요소

### Agent

- learners
- action을 취함 in environment에서, reward를 받음

### Policy function

- state에 대한 action
- agent의 행동에 대한 정의
- 파이 (symbol)
- lookup table

### value function

- 얼마나 좋은 state인가?
- state에 대한 value
- v(s)
- policy에 따라 달라질 수 있음
- optimal policy = optimal value function을 가짐

### model

- agent의 환경
- model-based(state가 어떻게 변경될지 알 고 있음- 길 찾는데 지도가 있음), model-free(시행착오에 기반- 지도가 없음)

# RL environment type

1. Deterministic env

- 다음 state가 확정적
- chess game
- action의 결과를 알고 있음

2. Stochastic env

- 다음 state 확정 x
- 주사위를 던질때 확률에 따라 정해짐

<br>

1. Fully observable env

- env를 모든 시간동안 관찰가능
- chess game

2. Partially observable env

- agent는 state를 모든 시간에서 결정 불가능
- poker game- 상대방의 카드를 모름

<br>

1. Discrete env

- 유한한 state - chess game

2. Continuous env

- 무한한 state - 자율주행

<br>

1. Episodic env

- 현재 action이 미래의 action에 영향 x
- final state 존재

2. Non-episodic env

- final state 존재 x
- 현재 action이 미래의 action에 영향

<br>

1. signle env : signle agent
2. multi env: multi agent
